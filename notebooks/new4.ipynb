{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display all the columns in output\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1044630\n",
      "2024-10-19 23:35:35,080 WARNING: using legacy validation callback\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (4.19s) \n",
      "Downloaded feature group: final_df_feature_group (version 1)\n"
     ]
    }
   ],
   "source": [
    "# Establish connection to Hopsworks using the API key\n",
    "project = hopsworks.login(\n",
    "    api_key_value=\"dYTVrTVvbj6Qw82i.YGKHdS9snQYFgOADJIvLdvZ2n2S5BxIAOtvPUEmAyd56bvaG6xhhGyNM3nYbexaP\"\n",
    ")\n",
    "\n",
    "# Access the Feature Store\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "# Specify the feature group and its version\n",
    "feature_group_name = \"final_df_feature_group\"\n",
    "feature_group_version = 1\n",
    "\n",
    "try:\n",
    "    # Retrieve the feature group\n",
    "    final_df_fg = fs.get_feature_group(feature_group_name, version=feature_group_version)\n",
    "    # Read the feature group as a Pandas DataFrame\n",
    "    final_df = final_df_fg.read()\n",
    "    print(f\"Downloaded feature group: {feature_group_name} (version {feature_group_version})\")\n",
    "except RestAPIError as e:\n",
    "    print(f\"Error downloading feature group: {feature_group_name} (version {feature_group_version})\")\n",
    "    raise e\n",
    "\n",
    "# Prepare the data\n",
    "unique_counts = final_df.nunique()\n",
    "cts_cols_df = final_df[['unique_id', 'route_avg_temp', 'route_avg_wind_speed', 'route_avg_precip',\n",
    "                         'route_avg_humidity', 'route_avg_visibility', 'route_avg_pressure', 'distance',\n",
    "                         'average_hours', 'temp_origin', 'wind_speed_origin', 'precip_origin',\n",
    "                         'humidity_origin', 'visibility_origin', 'pressure_origin',\n",
    "                         'temp_destination', 'wind_speed_destination', 'precip_destination',\n",
    "                         'humidity_destination', 'visibility_destination', 'pressure_destination',\n",
    "                         'avg_no_of_vehicles', 'truck_age', 'load_capacity_pounds', 'mileage_mpg',\n",
    "                         'age', 'experience', 'average_speed_mph']].copy()\n",
    "\n",
    "cat_cols_df = final_df[['route_description', 'description_origin', 'description_destination',\n",
    "                         'accident', 'fuel_type', 'gender', 'driving_style', \n",
    "                         'ratings', 'is_midnight']]\n",
    "\n",
    "date_cols_df = final_df[['unique_id', 'departure_date', 'estimated_arrival', \n",
    "                          'estimated_arrival_nearest_hour', 'departure_date_nearest_hour']].copy()\n",
    "\n",
    "target_df = final_df[['delay']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns\n",
    "cat_cols_encoded_df = pd.get_dummies(cat_cols_df, drop_first=True)\n",
    "\n",
    "# Concatenate the continuous columns, encoded categorical columns, and target column\n",
    "final_encoded_df = pd.concat([cts_cols_df, cat_cols_encoded_df, target_df], axis=1)\n",
    "\n",
    "# Scale the continuous columns\n",
    "unique_id = cts_cols_df['unique_id']\n",
    "cts_cols_without_id = cts_cols_df.drop('unique_id', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cts_cols_scaled = scaler.fit_transform(cts_cols_without_id)\n",
    "cts_cols_scaled_df = pd.DataFrame(cts_cols_scaled, columns=cts_cols_without_id.columns)\n",
    "cts_cols_scaled_df = pd.concat([unique_id.reset_index(drop=True), cts_cols_scaled_df], axis=1)\n",
    "\n",
    "final_scaled_df = pd.concat([cts_cols_scaled_df, cat_cols_encoded_df, target_df.reset_index(drop=True)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-19 23:36:04,738 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "delay\n",
      "0    6704\n",
      "1    6704\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Class Imbalance with Oversampling using SMOTE\n",
    "final_scaled_df_with_target = final_scaled_df.copy()\n",
    "X = final_scaled_df_with_target.drop(columns=['delay'])\n",
    "y = final_scaled_df_with_target['delay']\n",
    "\n",
    "# Apply SMOTE to create synthetic samples for the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Convert the resampled arrays back to a DataFrame\n",
    "final_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "final_resampled_df['delay'] = y_resampled\n",
    "\n",
    "# Verify the counts of the target variable after oversampling\n",
    "print(final_resampled_df['delay'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "unique_id = final_resampled_df['unique_id']\n",
    "target = final_resampled_df['delay']\n",
    "features_for_pca = final_resampled_df.drop(columns=['unique_id', 'delay'])\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "pca_transformed = pca.fit_transform(features_for_pca)\n",
    "pca_columns = [f'PCA_{i+1}' for i in range(pca_transformed.shape[1])]\n",
    "pca_df = pd.DataFrame(pca_transformed, columns=pca_columns)\n",
    "\n",
    "final_pca_df = pd.concat([unique_id.reset_index(drop=True), pca_df, target.reset_index(drop=True)], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (8850, 30), Validation set shape: (2572, 30), Test set shape: (1986, 30)\n"
     ]
    }
   ],
   "source": [
    "# Train, test, and validation split\n",
    "final_pca_df = final_pca_df.merge(date_cols_df[['unique_id', 'estimated_arrival']], on='unique_id', how='left')\n",
    "\n",
    "if 'estimated_arrival' in final_pca_df.columns:\n",
    "    final_pca_df['estimated_arrival'] = final_pca_df['estimated_arrival'].dt.tz_localize(None)\n",
    "\n",
    "    train_df = final_pca_df[final_pca_df['estimated_arrival'] <= pd.to_datetime('2019-01-30')]\n",
    "    validation_df = final_pca_df[(final_pca_df['estimated_arrival'] > pd.to_datetime('2019-01-30')) & \n",
    "                                  (final_pca_df['estimated_arrival'] <= pd.to_datetime('2019-02-07'))]\n",
    "    test_df = final_pca_df[final_pca_df['estimated_arrival'] > pd.to_datetime('2019-02-07')]\n",
    "\n",
    "    X_train = train_df.drop(columns=['delay', 'unique_id', 'estimated_arrival'])\n",
    "    y_train = train_df['delay']\n",
    "\n",
    "    X_valid = validation_df.drop(columns=['delay', 'unique_id', 'estimated_arrival'])\n",
    "    y_valid = validation_df['delay']\n",
    "\n",
    "    X_test = test_df.drop(columns=['delay', 'unique_id', 'estimated_arrival'])\n",
    "    y_test = test_df['delay']\n",
    "\n",
    "    # Print out the shapes of the resulting datasets to verify the splits\n",
    "    print(f\"Training set shape: {X_train.shape}, Validation set shape: {X_valid.shape}, Test set shape: {X_test.shape}\")\n",
    "else:\n",
    "    print(\"Error: 'estimated_arrival' column not found in final_pca_df.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.80      0.72      1244\n",
      "           1       0.77      0.62      0.68      1328\n",
      "\n",
      "    accuracy                           0.70      2572\n",
      "   macro avg       0.71      0.71      0.70      2572\n",
      "weighted avg       0.71      0.70      0.70      2572\n",
      "\n",
      "Random Forest Accuracy: 0.7045101088646968\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "rf_model = RandomForestClassifier(random_state = 42, max_depth=15, n_estimators= 200)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred_rf = rf_model.predict(X_valid)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred_rf))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_valid, y_valid_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68      1244\n",
      "           1       0.70      0.62      0.66      1328\n",
      "\n",
      "    accuracy                           0.67      2572\n",
      "   macro avg       0.67      0.67      0.67      2572\n",
      "weighted avg       0.67      0.67      0.67      2572\n",
      "\n",
      "Naive Bayes Accuracy: 0.6699066874027994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred_nb = nb_model.predict(X_valid)\n",
    "\n",
    "# Evaluate the Naive Bayes model\n",
    "print(\"Naive Bayes Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred_nb))\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_valid, y_valid_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      1244\n",
      "           1       0.71      0.58      0.64      1328\n",
      "\n",
      "    accuracy                           0.66      2572\n",
      "   macro avg       0.67      0.67      0.66      2572\n",
      "weighted avg       0.67      0.66      0.66      2572\n",
      "\n",
      "Logistic Regression Accuracy: 0.6632970451010887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train Logistic Regression model\n",
    "log_reg_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred_log_reg = log_reg_model.predict(X_valid)\n",
    "\n",
    "# Evaluate the Logistic Regression model\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred_log_reg))\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_valid, y_valid_pred_log_reg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-19 23:38:47,901 WARNING: UserWarning: [23:38:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.72      1244\n",
      "           1       0.75      0.68      0.71      1328\n",
      "\n",
      "    accuracy                           0.72      2572\n",
      "   macro avg       0.72      0.72      0.72      2572\n",
      "weighted avg       0.72      0.72      0.72      2572\n",
      "\n",
      "XGBoost Accuracy: 0.7192846034214619\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred_xgb = xgb_model.predict(X_valid)\n",
    "\n",
    "# Evaluate the XGBoost model\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred_xgb))\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_valid, y_valid_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "2024-10-20 00:39:28,606 WARNING: FitFailedWarning: \n",
      "540 fits failed out of a total of 1080.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "190 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\manik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\manik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\manik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\manik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "350 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\manik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\manik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\manik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\manik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "\n",
      "2024-10-20 00:39:28,718 WARNING: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.70949153 0.70836158 0.70960452\n",
      " 0.70915254 0.71062147 0.7120904  0.70610169 0.70576271 0.70621469\n",
      " 0.71062147 0.70881356 0.71163842 0.70451977 0.70723164 0.70711864\n",
      " 0.70610169 0.70655367 0.70485876 0.70508475 0.70519774 0.70485876\n",
      " 0.70508475 0.70519774 0.70485876 0.70361582 0.70497175 0.70610169\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.72700565 0.72960452 0.73389831\n",
      " 0.72508475 0.73050847 0.72915254 0.72497175 0.72723164 0.7300565\n",
      " 0.72463277 0.73118644 0.73096045 0.72553672 0.72779661 0.72903955\n",
      " 0.72090395 0.7240678  0.72576271 0.71785311 0.72225989 0.72564972\n",
      " 0.71785311 0.72225989 0.72564972 0.71841808 0.72022599 0.72237288\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.7299435  0.73491525 0.73909605\n",
      " 0.72971751 0.7320904  0.73389831 0.7280226  0.7299435  0.73423729\n",
      " 0.72892655 0.7319774  0.73581921 0.72497175 0.73322034 0.73423729\n",
      " 0.72824859 0.72836158 0.73118644 0.72632768 0.73186441 0.73254237\n",
      " 0.72632768 0.73186441 0.73254237 0.72259887 0.72519774 0.7280226\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.72485876 0.73412429 0.73785311\n",
      " 0.73050847 0.73548023 0.73751412 0.73412429 0.73378531 0.73457627\n",
      " 0.73559322 0.73706215 0.73762712 0.72983051 0.73118644 0.73096045\n",
      " 0.72610169 0.73084746 0.73276836 0.72689266 0.73175141 0.73242938\n",
      " 0.72689266 0.73175141 0.73242938 0.72429379 0.72587571 0.72926554]\n",
      "\n",
      "Best Random Forest Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best Random Forest Score: 0.7390960451977402\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.81      0.74      1244\n",
      "           1       0.78      0.63      0.70      1328\n",
      "\n",
      "    accuracy                           0.72      2572\n",
      "   macro avg       0.73      0.72      0.72      2572\n",
      "weighted avg       0.73      0.72      0.72      2572\n",
      "\n",
      "Random Forest Accuracy: 0.7204510108864697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Specify the hyperparameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [10, 15, 20, None],   # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],   # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],     # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['auto', 'sqrt'],   # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "rf_grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid,\n",
    "                               cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Random Forest Parameters:\", rf_grid_search.best_params_)\n",
    "print(\"Best Random Forest Score:\", rf_grid_search.best_score_)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred_rf = rf_grid_search.predict(X_valid)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred_rf))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_valid, y_valid_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define the model\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Specify the hyperparameter grid\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],    # Number of trees\n",
    "    'max_depth': [3, 5, 7, 9, 11],      # Maximum depth of the tree\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Step size shrinkage used in update to prevents overfitting\n",
    "    'subsample': [0.5, 0.75, 1.0],       # Fraction of samples to be used for each tree\n",
    "    'colsample_bytree': [0.5, 0.75, 1.0], # Fraction of features to be used for each tree\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "xgb_grid_search = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid,\n",
    "                                cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best XGBoost Parameters:\", xgb_grid_search.best_params_)\n",
    "print(\"Best XGBoost Score:\", xgb_grid_search.best_score_)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred_xgb = xgb_grid_search.predict(X_valid)\n",
    "\n",
    "# Evaluate the XGBoost model\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred_xgb))\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_valid, y_valid_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
